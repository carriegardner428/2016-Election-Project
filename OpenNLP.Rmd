---
title: "OpenNLP"
author: "Carrie Gardner"
date: "4/15/2017"
output: html_document
---

```{r setup, include=FALSE}
library(plyr)     # for recoding data
library(NLP)      # for NLP
library(tm)       # for Corpus()
library(SnowballC)
library(lsa)
library(NMF)
library(rJava)
library(openNLP)
```


```{r }
set.seed(1000)

load.data.task1 <- function() {

  user.tweets = read.csv('tweets_by_user.csv')
  users.party = read.csv('users.csv')
  
  user.tweets[1:3,]
  users.party[1:3,]
  
  data = merge(user.tweets, users.party, by=('userID'))
  data[1:3,]
  
    # if NAs exist, drop rows
  if (any(is.na(data))) {
    data = na.omit(data)
  }
  
  # Collapse parties
  table(data$party)
  data$party[data$party=="G"] = "I"
  data$party[data$party=="O"] = "I"
  data$party[data$party=="L"] = "I"
  table(data$party)
  
  dim(data) # 1,866 users (w/o party NAs)
  
# Change variable types to characters
  data$text = as.character(data$text)
  data$party = as.character(data$party)
  
  # Show data
  data[1:3,]
  return (data)
}


# Clean corpus
do.preprocess <- function(corpus, removeSparseTerms=FALSE) {
  # lowercase
  corpus = tm_map(corpus, content_transformer(tolower))
  # remove stops, punctuation, numbers, whitespaces; 

    # Remove everything besides english letters or space
  remov = function(x) gsub("[^[:alpha:][:space:]]*", "", x)
  corpus <- tm_map(corpus, content_transformer(remov))
  
  myStops = c(stopwords("english"), "https", "http", "debate", "debatenight", "debate night")
  corpus = tm_map(corpus, removeWords, myStops)
  corpus = tm_map(corpus, stripWhitespace)
  
  ## Optional - remove sparse terms
  if (removeSparseTerms) {
    corpus = tm_map(corpus, removeSparseTerms)
  }
  
  # remove URLs
  removeURL = function(x) gsub("http[^[:space:]]*", "", x)
  corpus = tm_map(corpus, removeURL)
  # stem
  # corpus = tm_map(corpus, stemDocument)
  
  return (corpus)
}
```

```{r}
# Tweets by User
user.tweets = load.data.task1()
```

```{r }
# User.tweets is a dataframe of unique users and a string of all of their tweets 
user.tweets[1:3,]
dim(user.tweets) # 1,866 users

# Get corpus
user.corpus = Corpus(VectorSource(user.tweets$text))
inspect(user.corpus[1:3])

# Preprocess corpus (lowercase; remove stops, punctuation, numbers, whitespaces; stem)
user.corpus = do.preprocess(user.corpus)
inspect(user.corpus[1:3])

## Finding Frequent Terms
tdm = TermDocumentMatrix(user.corpus)
findFreqTerms(tdm,lowfreq = 169)
 
# Make Term Document Matrix (tdm), where documents are user's Tweet Strings
user.tdm = TermDocumentMatrix(user.corpus,
                         control = list(
                                        stemming = F)
                         )
user.tdm
```


