---
title: "2016 Election"
author: "Carrie Gardner"
date: "3/29/2017"
output: html_document
---

```{r warning=FALSE, message=FALSE}
library(plyr)     # for recoding data
library(ggplot2)  # for plotting
library(NLP)      # for NLP
library(tm)       # for Corpus()
library(SnowballC)
library(lsa)
library(NMF)
```

##### Helper functions
```{r Helper }
set.seed(1000)

# Ingest datasets and cleans data
load.data.task <- function() {
  data.path = "user_setA/"
  
  data.files = c('tweets_debate1.csv', 'tweets_debate2.csv',
                 'tweets_debate3.csv', 'tweets_debateVP.csv')
  data.tweets = do.call(rbind,lapply(data.files, function(f) {
                            read.csv(f)
                })
         )
  
  data.users = read.csv('users.csv')
  
  data.tweets[1:3,]
  data.users[1:3,]
  data = merge(data.tweets, data.users, by=('userID'))
  
  data[1:3,]
  
  dim(data) # 60,378 observations
  # if NAs exist, drop rows
  if (any(is.na(data))) {
    data = na.omit(data)
  }
  dim(data) # now 38,044 observations
  
# Change variable types to characters
  data$text = as.character(data$text)
  data$party = as.character(data$party)
  
  # Show data
  data[1:3,]
  return (data)
}

# Plot Topic Histogram
do.topic_histo <- function(data) {
  ggplot(data, aes(x=factor(party), fill=party)) + 
    geom_bar(stat="count") + 
    scale_x_discrete("Party") + 
    scale_y_continuous("Frequency") + 
    scale_fill_manual(values=c('blue', 'green', 'purple', 'yellow', 'orange', 'red')) + 
    ggtitle("Party Frequency Across Tweets") +
    coord_flip()
}

# subset data and pull out Democrats, Republicans, and Independents
#### NEED TO UPDATE
get.subdoc <- function(data, toptopics=4) {
  selected_topics = sort(table(data$Topic), decreasing = T)[1:toptopics]
  selected_topics = names(selected_topics)
  selected_topics
  
  doc_idx = which(data$Topic %in% selected_topics)
  subdoc = data[doc_idx,]
  return(subdoc)
}

# Clean corpus
do.preprocess <- function(corpus, stem=FALSE, removeSparseTerms=FALSE) {
  # lowercase
  corpus = tm_map(corpus, content_transformer(tolower))
  # remove stops, punctuation, numbers, whitespaces; 
  corpus = tm_map(corpus, function(x) removeWords(x, stopwords("english")))
  corpus = tm_map(corpus, removePunctuation)
  corpus = tm_map(corpus, removeNumbers)
  corpus = tm_map(corpus, stripWhitespace)
  ## Optional - remove sparse terms
  if (removeSparseTerms) {
    corpus = tm_map(corpus, removeSparseTerms)
  }
  # stem
  if (stem) {
    corpus = tm_map(corpus, stemDocument, language = "english")
  }
  return (corpus)
}

# NMF
do.nmf <- function(tdm) {
  nmf.res = nmf(tdm, 3, "lee")
  # estimate target matrix
  v.hat = fitted(nmf.res)
  # w = n * r term feature matrix
  w = basis(nmf.res)
  h = coef(nmf.res)
  return(h)
}

do.mds <- function(tdm) {
  # compute distance matrix
  tdm = as.matrix(tdm)
  tdm.dist = dist(t(tdm)) 
  tdm.mds = cmdscale(tdm.dist, k = 2)
  
  return(tdm.mds)
}

```

# Read in data 
# Examine class Distribution
```{r }
data = load.data.task()
dim(data)
```

# Distribution of Tweets
```{r}
do.topic_histo(data)
## Since the data is heavily unbalanced, perhaps we should downsample to remove Democratic tweets?
```

## Collapse Other, Libertarian Party, and Green Party into Independent category.
```{r}
table(data$party)
# Relabel
data$party[data$party=="G"] = "I"
data$party[data$party=="O"] = "I"
data$party[data$party=="L"] = "I"

table(data$party)
```

## Replot Tweet Class Distribution
```{r}
ggplot(data, aes(x=factor(party), fill=party)) + 
    geom_bar(stat="count") + 
    scale_x_discrete("Party") + 
    scale_y_continuous("Frequency") + 
    scale_fill_manual(values=c('blue', 'purple','red')) + 
    ggtitle("Party Frequency Across Tweets") +
    coord_flip()

```
# Examine Distribution of Users
```{r}
data.users = read.csv('users.csv')
# Plot Histogram
ggplot(data.users, aes(x=factor(party), fill=party)) + 
    geom_bar(stat="count") + 
    scale_x_discrete("Party") + 
    scale_y_continuous("Frequency") + 
    scale_fill_manual(values=c('blue', 'green', 'purple', 'yellow', 'orange', 'red'), na.value="grey50") + 
    ggtitle("Party Frequency Across Users") +
    coord_flip()

# dim(data.users)
# 22,853 users
```

```{r}
table(data.users$party)

## Collapse G, L, and O into Independent Party

# Relabel
data.users$party[data.users$party=="G"] = "I"
data.users$party[data.users$party=="O"] = "I"
data.users$party[data.users$party=="L"] = "I"

data.users$party = droplevels(data.users$party)
table(data.users$party)
```

## Replot User Class Distribution
```{r}
ggplot(data.users, aes(x=factor(party), fill=party)) + 
    geom_bar(stat="count") + 
    scale_x_discrete("Party") + 
    scale_y_continuous("Frequency") + 
    scale_fill_manual(values=c('blue', 'purple','red'), na.value="grey50") + 
    ggtitle("Party Frequency Across Users") +
    coord_flip()

```
# Handle NAs w/Heuristic to Label Class by 2016 State Electoral College Choice
```{r}
table(data.users$state_code)

# Recode data
# data.users$party[which(data.users$party == "NA" &  ]


data.users$party[data.users$party=="G"] = "I"

gop_corpus = data[which(data$party == "R"),]

```

# US Choropleth
```{r}
## Reference: https://plot.ly/r/choropleth-maps/#choropleth-maps-in-r

##### TO DO :  NEED TO GET DATA IN PROPER FORMAT...df = State Code, Dem Count, GOP Count, Ind Count, Electoral College Winner

data.map = unique(data.users$state_code)
data.map$

str(data.map)

levels = table(data.map$party)
data.map$Dem = levels[1]
data.map$Dem = 

data.map$hover = with(data.map, paste(state_code, party))



library(plotly)
df <- read.csv("https://raw.githubusercontent.com/plotly/datasets/master/2011_us_ag_exports.csv")


df$hover <- with(df, paste(state, '<br>', "Beef", beef, "Dairy", dairy, "<br>",
                           "Fruits", total.fruits, "Veggies", total.veggies,
                           "<br>", "Wheat", wheat, "Corn", corn))
# give state boundaries a white border
l <- list(color = toRGB("white"), width = 2)
# specify some map projection/options
g <- list(
  scope = 'usa',
  projection = list(type = 'albers usa'),
  showlakes = TRUE,
  lakecolor = toRGB('white')
)

p <- plot_geo(df, locationmode = 'USA-states') %>%
  add_trace(
    z = ~total.exports, text = ~hover, locations = ~code,
    color = ~total.exports, colors = 'Purples'
  ) %>%
  colorbar(title = "Millions USD") %>%
  layout(
    title = '2011 US Agriculture Exports by State<br>(Hover for breakdown)',
    geo = g
  )

p

```


## Word Cloud
#### Reference: https://www.r-bloggers.com/building-wordclouds-in-r/
```{r}
library(wordcloud)
library(RColorBrewer)
corpus = Corpus(VectorSource(data$text))

corpus = do.preprocess(corpus, stem=FALSE)
inspect(corpus[10:30])

# Make Term Document Matrix (tdm)
tdm = TermDocumentMatrix(corpus)

dim(tdm)

wordcloud(corpus, max.words = 100, random.order = FALSE)

# GOP Corpus
gop_corpus = data[which(data$party == "R"),]
gop_corpus = Corpus(VectorSource(gop_corpus$text))
gop_corpus = do.preprocess(gop_corpus, stem=FALSE)
wordcloud(gop_corpus, max.words = 100, random.order = FALSE, colors="Red")
  
# Dem Corpus
dem_corpus = data[which(data$party == "D"),]
dem_corpus = Corpus(VectorSource(dem_corpus$text))
dem_corpus = do.preprocess(dem_corpus, stem=FALSE)
wordcloud(dem_corpus, max.words = 100, random.order = FALSE, colors="Blue")

# Independent Corpus
ind_corpus = data[which(data$party == "I"),]
ind_corpus = Corpus(VectorSource(ind_corpus$text))
ind_corpus = do.preprocess(ind_corpus, stem=FALSE)
wordcloud(ind_corpus, max.words = 100, random.order = FALSE, colors="Purple")

```

## Extract Hashtags

```{r}
library(stringr)

pat = "(#\\S+)"
data = transform(data,hashtags=stringr::str_extract(text,pat))

hashtags = data$hashtags
hashtags = na.omit(hashtags)

top_hashtags = sort(table(hashtags), decreasing = T)
barplot(top_hashtags[1:10],horiz=TRUE, 
        main="Top Hashtags", 
  	    xlab="Counts", 
  	    cex.names=0.5, las=1)

wordcloud(hashtags, max.words = 100, random.order = FALSE)

# GOP Hashtags
gop_hashtags = data[which(data$party == "R"),]
gop_hashtags = gop_hashtags$hashtags
wordcloud(gop_hashtags, max.words = 100, random.order = FALSE, colors="Red")

## extract top 10 GOP Hashtags
gop_top10 = sort(table(gop_hashtags), decreasing = T)
# Plot most frequent 
barplot(gop_top10[1:10],horiz=TRUE, 
        main="GOP Hashtags", 
  	    xlab="Counts", 
  	    cex.names=0.5, las=1)

# Dem Hashtags
dem_hashtags = data[which(data$party == "D"),]
dem_hashtags = dem_hashtags$hashtags
wordcloud(dem_hashtags, max.words = 100, random.order = FALSE, colors="Blue")

## extract top 10 Dem Hashtags
dem_top10 = sort(table(dem_hashtags), decreasing = T)
# Plot most frequent 
barplot(dem_top10[1:10],horiz=TRUE, 
        main="Dem Hashtags", 
  	    xlab="Counts", 
        cex.names=0.5, las=1)

# Independent Hashtags
ind_hashtags = data[which(data$party == "I"),]
ind_hashtags = ind_hashtags$hashtags
wordcloud(ind_hashtags, max.words = 100, random.order = FALSE, colors="Purple")



```

```{r}

wordcloud(hashtags, max.words = 100, random.order = FALSE)


corpus = Corpus(VectorSource(data$hashtags))
wordcloud(corpus, max.words = 100, random.order = FALSE)

```

```{r}
# Extract Retweets

pat = "(RT @\\S+)"
data = transform(data,retweets=stringr::str_extract(text,pat))

retweets = data$retweets
retweets = na.omit(retweets)
retweets

length(retweets)
# 19434 tweets are retweets
```

```{r}
# Extract Tweets w/Mentions (@)
pat = "(@\\S+)"

data = transform(data,mentions=stringr::str_extract(text,pat))

mentions = data$mentions
mentions = na.omit(mentions)
mentions

```


```{r}
# Most Favorited Tweet




