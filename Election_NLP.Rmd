---
title: "2016 Election"
author: "Carrie Gardner"
date: "3/29/2017"
output: html_document
---

```{r warning=FALSE, message=FALSE}
library(plyr)     # for recoding data
library(ggplot2)  # for plotting
library(NLP)      # for NLP
library(tm)       # for Corpus()
library(SnowballC)
library(lsa)
library(NMF)
```

##### Helper functions
```{r Helper }
set.seed(1000)

# Ingest datasets and cleans data
load.data.task <- function() {
  data.path = "user_setA/"
  
  data.files = c('tweets_debate1.csv', 'tweets_debate2.csv',
                 'tweets_debate3.csv', 'tweets_debateVP.csv')
  data.tweets = do.call(rbind,lapply(data.files, function(f) {
                            read.csv(f)
                })
         )
  
  data.users = read.csv('users.csv')
  
  data.tweets[1:3,]
  data.users[1:3,]
  data = merge(data.tweets, data.users, by=('userID'))
  
  data[1:3,]
  
  dim(data) # 60,378 observations
  # if NAs exist, drop rows
  if (any(is.na(data))) {
    data = na.omit(data)
  }
  dim(data) # now 38,044 observations
  
# Change variable types to characters
  data$text = as.character(data$text)
  data$party = as.character(data$party)
  
  # Show data
  data[1:3,]
  return (data)
}

# Plot Topic Histogram
do.topic_histo <- function(data) {
  ggplot(data, aes(x=factor(party), fill=party)) + 
    geom_bar(stat="count") + 
    scale_x_discrete("Party") + 
    scale_y_continuous("Frequency") + 
    scale_fill_manual(values=c('blue', 'green', 'purple', 'yellow', 'orange', 'red')) + 
    ggtitle("Party Frequency") +
    coord_flip()
}

# subset data and pull out Democrats, Republicans, and Independents
#### NEED TO UPDATE
get.subdoc <- function(data, toptopics=4) {
  selected_topics = sort(table(data$Topic), decreasing = T)[1:toptopics]
  selected_topics = names(selected_topics)
  selected_topics
  
  doc_idx = which(data$Topic %in% selected_topics)
  subdoc = data[doc_idx,]
  return(subdoc)
}

# Clean corpus
do.preprocess <- function(corpus, stem=FALSE, removeSparseTerms=FALSE) {
  # lowercase
  corpus = tm_map(corpus, content_transformer(tolower))
  # remove stops, punctuation, numbers, whitespaces; 
  corpus = tm_map(corpus, function(x) removeWords(x, stopwords("english")))
  corpus = tm_map(corpus, removePunctuation)
  corpus = tm_map(corpus, removeNumbers)
  corpus = tm_map(corpus, stripWhitespace)
  ## Optional - remove sparse terms
  if (removeSparseTerms) {
    corpus = tm_map(corpus, removeSparseTerms)
  }
  # stem
  if (stem) {
    corpus = tm_map(corpus, stemDocument, language = "english")
  }
  return (corpus)
}

# NMF
do.nmf <- function(tdm) {
  nmf.res = nmf(tdm, 3, "lee")
  # estimate target matrix
  v.hat = fitted(nmf.res)
  # w = n * r term feature matrix
  w = basis(nmf.res)
  h = coef(nmf.res)
  return(h)
}

do.mds <- function(tdm) {
  # compute distance matrix
  tdm = as.matrix(tdm)
  tdm.dist = dist(t(tdm)) 
  tdm.mds = cmdscale(tdm.dist, k = 2)
  
  return(tdm.mds)
}

```

# Read in data 
# Examine class Distribution
```{r }
data = load.data.task()
dim(data)

do.topic_histo(data)

## Since the data is heavily unbalanced, perhaps we should randomly remove some Democrats?
```

Collapse Other, Libertarian Party, and Green Party into Independent category.

```{r}
table(data$party)

data$party[data$party=="G"] = "I"
data$party[data$party=="O"] = "I"
data$party[data$party=="L"] = "I"

table(data$party)

ggplot(data, aes(x=factor(party), fill=party)) + 
    geom_bar(stat="count") + 
    scale_x_discrete("Party") + 
    scale_y_continuous("Frequency") + 
    scale_fill_manual(values=c('blue', 'purple','red')) + 
    ggtitle("Party Frequency") +
    coord_flip()
  
# Plot Topic Histogram
do.topic_histo <- function(data) {

}

```

## Word Cloud
#### Reference: https://www.r-bloggers.com/building-wordclouds-in-r/
```{r}
library(wordcloud)
library(RColorBrewer)
corpus = Corpus(VectorSource(data$text))

corpus = do.preprocess(corpus, stem=FALSE)
inspect(corpus[10:30])

# Make Term Document Matrix (tdm)
tdm = TermDocumentMatrix(corpus)

wordcloud(corpus, max.words = 100, random.order = FALSE)
wordcloud(corpus, max.words = 100, random.order = FALSE, colors=brewer.pal(8, "Dark2"))
wordcloud(corpus, max.words = 100, random.order = FALSE, colors="Purple")


# GOP Corpus
gop_corpus = data[which(data$party == "R"),]
gop_corpus = Corpus(VectorSource(gop_corpus$text))
gop_corpus = do.preprocess(gop_corpus, stem=FALSE)
wordcloud(gop_corpus, max.words = 100, random.order = FALSE, colors="Red")
  
# Dem Corpus
dem_corpus = data[which(data$party == "D"),]
dem_corpus = Corpus(VectorSource(dem_corpus$text))
dem_corpus = do.preprocess(dem_corpus, stem=FALSE)
wordcloud(dem_corpus, max.words = 100, random.order = FALSE, colors="Blue")

# Independent Corpus
ind_corpus = data[which(data$party == "I"),]
ind_corpus = Corpus(VectorSource(ind_corpus$text))
ind_corpus = do.preprocess(ind_corpus, stem=FALSE)
wordcloud(ind_corpus, max.words = 100, random.order = FALSE, colors="Purple")

```

## Extract Hashtags

```{r}
library(stringr)

pat = "(#\\S+)"
data = transform(data,hashtags=stringr::str_extract(text,pat))

hashtags = data$hashtags
hashtags = na.omit(hashtags)

top_hashtags = sort(table(hashtags), decreasing = T)
barplot(top_hashtags[1:10],horiz=TRUE, 
        main="Top Hashtags", 
  	    xlab="Counts", 
  	    cex.names=0.5, las=1)


# GOP Hashtags
gop_hashtags = data[which(data$party == "R"),]
gop_hashtags = gop_hashtags$hashtags
wordcloud(gop_hashtags, max.words = 100, random.order = FALSE, colors="Red")

## extract top 10 GOP Hashtags
gop_top10 = sort(table(gop_hashtags), decreasing = T)
# Plot most frequent 
barplot(gop_top10[1:10],horiz=TRUE, 
        main="GOP Hashtags", 
  	    xlab="Counts", 
  	    cex.names=0.5, las=1)

# Dem Hashtags
dem_hashtags = data[which(data$party == "D"),]
dem_hashtags = dem_hashtags$hashtags
wordcloud(dem_hashtags, max.words = 100, random.order = FALSE, colors="Blue")

## extract top 10 Dem Hashtags
dem_top10 = sort(table(dem_hashtags), decreasing = T)
# Plot most frequent 
barplot(dem_top10[1:10],horiz=TRUE, 
        main="Dem Hashtags", 
  	    xlab="Counts", 
        cex.names=0.5, las=1)

# Independent Hashtags
ind_hashtags = data[which(data$party == "I"),]
ind_hashtags = ind_hashtags$hashtags
wordcloud(ind_hashtags, max.words = 100, random.order = FALSE, colors="Purple")



```

```{r}

wordcloud(hashtags, max.words = 100, random.order = FALSE)


corpus = Corpus(VectorSource(data$hashtags))
wordcloud(corpus, max.words = 100, random.order = FALSE)

```
