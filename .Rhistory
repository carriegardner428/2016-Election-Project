xlab="Number of Gears")
barplot(gop_top10, main="Car Distribution",
xlab="Number of Gears")
barplot(gop_top10[1:10], main="Car Distribution",
xlab="Number of Gears")
barplot(gop_top10[1:5],horiz=TRUE, main="Car Distribution",
xlab="Number of Gears")
barplot(gop_top10[1:5],horiz=TRUE, main="Car Distribution",
xlab="Number of Gears", cex.names=0.8)
barplot(gop_top10[1:5],horiz=TRUE, main="Car Distribution",
xlab="Number of Gears", cex.names=0.5)
barplot(gop_top10[1:10],horiz=TRUE, main="GOP Hashtags",
xlab="Counts", cex.names=0.5)
barplot(gop_top10[1:10],horiz=TRUE, main="GOP Hashtags",
xlab="Counts", cex.names=0.5, las=1)
dem_top10 = sort(table(dem_hashtags), decreasing = T)
barplot(dem_top10[1:10],horiz=TRUE,
main="Dem Hashtags",
xlab="Counts",
cex.names=0.5, las=1)
barplot(hashtags[1:10],horiz=TRUE,
main="Top Hashtags",
xlab="Counts",
cex.names=0.5, las=1)
top_hashtags = sort(table(hashtags), decreasing = T)
barplot(top_hashtags[1:10],horiz=TRUE,
main="Top Hashtags",
xlab="Counts",
cex.names=0.5, las=1)
pat = "(rt\\S+)"
data.retweets = transform(data,hashtags=stringr::str_extract(text,pat))
dim(data.retweets)
dim(data)
dim(data)
dim(data.retweets)
pat = "(RT\\S+)"
data.retweets = transform(data,hashtags=stringr::str_extract(text,pat))
dim(data.retweets)
dim(data)
View(data.top4)
View(data)
pat = "(RT @\\S+)"
data.retweets = transform(data,hashtags=stringr::str_extract(text,pat))
dim(data.retweets)
dim(data)
View(data.retweets)
txt <- c("arm","foot","lefroo", "bafoobar")
if(any(i <- grep("foo",txt)))
cat("'foo' appears at least once in\n\t",txt,"\n")
i # 2 and 4
txt[i]
if(any(i = grep("RT @", data)))
cat("Yes")
i
data[i]
dim(i)
str(i)
i
i[1]
data[i]
pat = "(RT @\\S+)"
data.retweets = transform(data,retweets=stringr::str_extract(text,pat))
retweets = data$retweets
retweets
pat = "(RT @\\S+)"
data.retweets = transform(data,retweets=stringr::str_extract(text,pat))
retweets = data$retweets
retweets
pat = "(RT @\\S+)"
data$retweets = transform(data,retweets=stringr::str_extract(text,pat))
retweets = data$retweets
retweets
pat = "(RT @\\S+)"
data = transform(data,retweets=stringr::str_extract(text,pat))
retweets = data$retweets
retweets
retweets = na.omit(retweets)
retweets
dim(retweets)
len(retweets)
length(retweets)
pat = "(@*:\\S+)"
data = transform(data,mentions=stringr::str_extract(text,pat))
mentions = data$mentions
mentions = na.omit(mentions)
mentions
retweets
pat = "(@:alnum::\\S+)"
data = transform(data,mentions=stringr::str_extract(text,pat))
mentions = data$mentions
mentions = na.omit(mentions)
mentions
pat = "(@[:alnum:]:\\S+)"
data = transform(data,mentions=stringr::str_extract(text,pat))
mentions = data$mentions
mentions = na.omit(mentions)
mentions
pat = "(@[[:graph:]]:\\S+)"
data = transform(data,mentions=stringr::str_extract(text,pat))
mentions = data$mentions
mentions = na.omit(mentions)
mentions
pat = "(@\w+ )"
pat = "(@\w+)"
pat = "(@\w+/i)"
pat = "(\s([@#][\w_-]+))"
pat = "(\s([@#][\w_-]+)"
pat = "([@#][\w_-]+)"
pat = "(@\\S+)"
data = transform(data,mentions=stringr::str_extract(text,pat))
mentions = data$mentions
mentions = na.omit(mentions)
mentions
pat = "(@\\S+\\:\\S+)"
data = transform(data,mentions=stringr::str_extract(text,pat))
mentions = data$mentions
mentions = na.omit(mentions)
mentions
pat = "(@\\S+\\:S+)"
data = transform(data,mentions=stringr::str_extract(text,pat))
mentions = data$mentions
mentions = na.omit(mentions)
mentions
pat = "(@\\S+)"
data = transform(data,mentions=stringr::str_extract(text,pat))
mentions = data$mentions
mentions = na.omit(mentions)
mentions
tdm = TermDocumentMatrix(corpus)
dim(tdm)
View(data)
View(data)
table(data$userID)
dim(tdm)
ggplot(data, aes(x=factor(party), fill=party)) +
geom_bar(stat="count") +
scale_x_discrete("Party") +
scale_y_continuous("Frequency") +
scale_fill_manual(values=c('blue', 'purple','red')) +
ggtitle("Party Frequency") +
coord_flip()
data.users = read.csv('users.csv')
ggplot(data.users, aes(x=factor(party), fill=party)) +
geom_bar(stat="count") +
scale_x_discrete("Party") +
scale_y_continuous("Frequency") +
# scale_fill_manual(values=c('blue', 'purple','red')) +
ggtitle("Party Frequency") +
coord_flip()
dim(data.users)
ggplot(data.users, aes(x=factor(party), fill=party)) +
geom_bar(stat="count") +
scale_x_discrete("Party") +
scale_y_continuous("Frequency") +
# scale_fill_manual(values=c('blue', 'purple','red')) +
ggtitle("Party Frequency") +
coord_flip()
ggplot(data, aes(x=factor(party), fill=party)) +
geom_bar(stat="count") +
scale_x_discrete("Party") +
scale_y_continuous("Frequency") +
scale_fill_manual(values=c('blue', 'purple','red')) +
ggtitle("Party Frequency") +
coord_flip()
ggplot(data.users, aes(x=factor(party), fill=party)) +
geom_bar(stat="count") +
scale_x_discrete("Party") +
scale_y_continuous("Frequency") +
# scale_fill_manual(values=c('blue', 'purple','red')) +
ggtitle("Party Frequency") +
coord_flip()
ggplot(data.users, aes(x=factor(party), fill=party)) +
geom_bar(stat="count") +
scale_x_discrete("Party") +
scale_y_continuous("Frequency") +
scale_fill_manual(values=c('gray', 'blue', 'green', 'purple', 'yellow', 'orange', 'red')) +
ggtitle("Party Frequency") +
coord_flip()
ggplot(data.users, aes(x=factor(party), fill=party)) +
geom_bar(stat="count") +
scale_x_discrete("Party") +
scale_y_continuous("Frequency") +
scale_fill_manual(values=c('black', 'blue', 'green', 'purple', 'yellow', 'orange', 'red')) +
ggtitle("Party Frequency") +
coord_flip()
ggplot(data.users, aes(x=factor(party), fill=party)) +
geom_bar(stat="count") +
scale_x_discrete("Party") +
scale_y_continuous("Frequency") +
scale_fill_manual(values=c('blue', 'green', 'purple', 'yellow', 'orange', 'red', 'black')) +
ggtitle("Party Frequency") +
coord_flip()
ggplot(data.users, aes(x=factor(party), fill=party)) +
geom_bar(stat="count") +
scale_x_discrete("Party") +
scale_y_continuous("Frequency") +
scale_fill_manual(values=c('blue', 'green', 'purple', 'yellow', 'orange', 'red')) +
ggtitle("Party Frequency") +
coord_flip()
ggplot(data.users, aes(x=factor(party), fill=party)) +
geom_bar(stat="count") +
scale_x_discrete("Party") +
scale_y_continuous("Frequency") +
# scale_fill_manual(values=c('blue', 'green', 'purple', 'yellow', 'orange', 'red')) +
ggtitle("Party Frequency") +
coord_flip()
table(data.users$party)
ggplot(data.users, aes(x=factor(party), fill=party)) +
geom_bar(stat="count") +
scale_x_discrete("Party") +
scale_y_continuous("Frequency") +
# scale_fill_manual(values=c('blue', 'green', 'purple', 'yellow', 'orange', 'red')) +
ggtitle("Party Frequency Across Users") +
coord_flip()
ggplot(data.users, aes(x=factor(party), fill=party)) +
geom_bar(stat="count") +
scale_x_discrete("Party") +
scale_y_continuous("Frequency") +
scale_fill_manual(values=c('blue', 'green', 'purple', 'yellow', 'orange', 'red'), na.value = "grey50") +
ggtitle("Party Frequency Across Users") +
coord_flip()
data.users$party[data$party=="G"] = "I"
data.users$party[data.users$party=="G"] = "I"
data.users$party[data.users$party=="O"] = "I"
data.users$party[data.users$party=="L"] = "I"
table(data.users$party)
data.users$party = droplevel(data.users$party)
data.users$party = droplevels(data.users$party)
data.users$party = droplevels(data.users$party)
table(data.users$party)
table(data$party)
ggplot(data.users, aes(x=factor(party), fill=party)) +
geom_bar(stat="count") +
scale_x_discrete("Party") +
scale_y_continuous("Frequency") +
scale_fill_manual(values=c('blue', 'purple','red')) +
ggtitle("Party Frequency Across Users") +
coord_flip()
ggplot(data.users, aes(x=factor(party), fill=party)) +
geom_bar(stat="count") +
scale_x_discrete("Party") +
scale_y_continuous("Frequency") +
scale_fill_manual(values=c('blue', 'purple','red'), na.value="grey50") +
ggtitle("Party Frequency Across Users") +
coord_flip()
table(data.users$state_code)
corpus = Corpus(VectorSource(data$text))
corpus = do.preprocess(corpus, stem=FALSE)
inspect(corpus[10:30])
tdm = TermDocumentMatrix(corpus)
wordcloud(corpus, max.words = 100, random.order = FALSE)
wordcloud(corpus, max.words = 100, random.order = FALSE, random.color=FALSE)
wordcloud(corpus, max.words = 100, random.order = FALSE, random.color=FALSE)
wordcloud(corpus, max.words = 100, random.order = FALSE, random.color=FALSE)
wordcloud(corpus, max.words = 100, random.order = FALSE, ordered.colors = TRUE)
top_hashtags = sort(table(hashtags), decreasing = T)
barplot(top_hashtags[1:10],horiz=TRUE,
main="Top Hashtags",
xlab="Counts",
cex.names=0.5, las=1)
wordcloud(top_hashtags, max.words = 100, random.order = FALSE, colors="Red")
pat = "(#\\S+)"
data = transform(data,hashtags=stringr::str_extract(text,pat))
hashtags = data$hashtags
hashtags = na.omit(hashtags)
top_hashtags = sort(table(hashtags), decreasing = T)
wordcloud(top_hashtags, max.words = 100, random.order = FALSE, colors="Red")
wordcloud(hashtags, max.words = 100, random.order = FALSE, colors="Red")
wordcloud(hashtags, max.words = 100, random.order = FALSE)
wordcloud(dem_hashtags, max.words = 100, random.order = FALSE, colors="Blue")
wordcloud(gop_hashtags, max.words = 100, random.order = FALSE, colors="Red")
wordcloud(gop_corpus, max.words = 100, random.order = FALSE, colors="Red")
wordcloud(dem_corpus, max.words = 100, random.order = FALSE, colors="Blue")
library(ggplot2)
library(fiftystater)
install.packages('fiftystater')
function (package, help, pos = 2, lib.loc = NULL, character.only = FALSE,
logical.return = FALSE, warn.conflicts = TRUE, quietly = FALSE,
verbose = getOption("verbose"))
library(fiftystater)
data("fifty_states") # this line is optional due to lazy data loading
library(ggplot2)
library(fiftystater)
data("fifty_states") # this line is optional due to lazy data loading
crimes <- data.frame(state = tolower(rownames(USArrests)), USArrests)
p <- ggplot(data.users, aes(map_id = state_code)) +
# map points to the fifty_states shape data
geom_map(aes(fill = party), map = fifty_states) +
expand_limits(x = fifty_states$long, y = fifty_states$lat) +
coord_map() +
scale_x_continuous(breaks = NULL) +
scale_y_continuous(breaks = NULL) +
labs(x = "", y = "") +
theme(legend.position = "bottom",
panel.background = element_blank())
p
install.packages('mapproj')
p
p <- ggplot(data.users, aes(map_id = state_code)) +
# map points to the fifty_states shape data
geom_map(aes(fill = party), map = fifty_states) +
expand_limits(x = fifty_states$long, y = fifty_states$lat) +
coord_map() +
scale_x_continuous(breaks = NULL) +
scale_y_continuous(breaks = NULL) +
labs(x = "", y = "") +
theme(legend.position = "bottom",
panel.background = element_blank())
p
p + fifty_states_inset_boxes()
library(plotly)
df <- read.csv("https://raw.githubusercontent.com/plotly/datasets/master/2011_us_ag_exports.csv")
df$hover <- with(df, paste(state, '<br>', "Beef", beef, "Dairy", dairy, "<br>",
"Fruits", total.fruits, "Veggies", total.veggies,
"<br>", "Wheat", wheat, "Corn", corn))
l <- list(color = toRGB("white"), width = 2)
g <- list(
scope = 'usa',
projection = list(type = 'albers usa'),
showlakes = TRUE,
lakecolor = toRGB('white')
)
p <- plot_geo(df, locationmode = 'USA-states') %>%
add_trace(
z = ~total.exports, text = ~hover, locations = ~code,
color = ~total.exports, colors = 'Purples'
) %>%
colorbar(title = "Millions USD") %>%
layout(
title = '2011 US Agriculture Exports by State<br>(Hover for breakdown)',
geo = g
)
chart_link = plotly_POST(p, filename="choropleth/ag")
p
View(df)
View(data.users)
data.map <- data.users
View(data.map)
table(data.map$party)
levels = table(data.map$party)
levels[1]
data.map <- data.users$state_code
data.map
data.map <- unique(data.users$state_code)
data.map
dim(data.map)
levels(data.map)
str(data.map)
table(data.users$state_code)
# Recode data
# data.users$party[which(data.users$party == "NA" &  ]
data.users$party[data.users$party=="G"] = "I"
gop_corpus = data[which(data$party == "R"),]
library(plyr)     # for recoding data
library(ggplot2)  # for plotting
library(NLP)      # for NLP
library(tm)       # for Corpus()
library(SnowballC)
library(lsa)
library(NMF)
set.seed(1000)
# Ingest datasets and cleans data
load.data.task <- function() {
data.path = "user_setA/"
data.files = c('tweets_debate1.csv', 'tweets_debate2.csv',
'tweets_debate3.csv', 'tweets_debateVP.csv')
data.tweets = do.call(rbind,lapply(data.files, function(f) {
read.csv(f)
})
)
data.users = read.csv('users.csv')
data.tweets[1:3,]
data.users[1:3,]
data = merge(data.tweets, data.users, by=('userID'))
data[1:3,]
dim(data) # 60,378 observations
# if NAs exist, drop rows
if (any(is.na(data))) {
data = na.omit(data)
}
dim(data) # now 38,044 observations
# Change variable types to characters
data$text = as.character(data$text)
data$party = as.character(data$party)
# Show data
data[1:3,]
return (data)
}
# subset data and pull out Democrats, Republicans, and Independents
#### NEED TO UPDATE
get.subdoc <- function(data, toptopics=4) {
selected_topics = sort(table(data$Topic), decreasing = T)[1:toptopics]
selected_topics = names(selected_topics)
selected_topics
doc_idx = which(data$Topic %in% selected_topics)
subdoc = data[doc_idx,]
return(subdoc)
}
# Clean corpus
do.preprocess <- function(corpus, stem=FALSE, removeSparseTerms=FALSE) {
# lowercase
corpus = tm_map(corpus, content_transformer(tolower))
# remove stops, punctuation, numbers, whitespaces;
corpus = tm_map(corpus, function(x) removeWords(x, stopwords("english")))
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, stripWhitespace)
## Optional - remove sparse terms
if (removeSparseTerms) {
corpus = tm_map(corpus, removeSparseTerms)
}
# stem
if (stem) {
corpus = tm_map(corpus, stemDocument, language = "english")
}
return (corpus)
}
# NMF
do.nmf <- function(tdm) {
nmf.res = nmf(tdm, 3, "lee")
# estimate target matrix
v.hat = fitted(nmf.res)
# w = n * r term feature matrix
w = basis(nmf.res)
h = coef(nmf.res)
return(h)
}
do.mds <- function(tdm) {
# compute distance matrix
tdm = as.matrix(tdm)
tdm.dist = dist(t(tdm))
tdm.mds = cmdscale(tdm.dist, k = 2)
return(tdm.mds)
}
data = load.data.task()
setwd("~/Documents/School/iSchool/Data Mining/Final/2016 Election Tweets/user_setA")
set.seed(1000)
# Ingest datasets and cleans data
load.data.task <- function() {
data.path = "user_setA/"
data.files = c('tweets_debate1.csv', 'tweets_debate2.csv',
'tweets_debate3.csv', 'tweets_debateVP.csv')
data.tweets = do.call(rbind,lapply(data.files, function(f) {
read.csv(f)
})
)
data.users = read.csv('users.csv')
data.tweets[1:3,]
data.users[1:3,]
data = merge(data.tweets, data.users, by=('userID'))
data[1:3,]
dim(data) # 60,378 observations
# if NAs exist, drop rows
if (any(is.na(data))) {
data = na.omit(data)
}
dim(data) # now 38,044 observations
# Change variable types to characters
data$text = as.character(data$text)
data$party = as.character(data$party)
# Show data
data[1:3,]
return (data)
}
# subset data and pull out Democrats, Republicans, and Independents
#### NEED TO UPDATE
get.subdoc <- function(data, toptopics=4) {
selected_topics = sort(table(data$Topic), decreasing = T)[1:toptopics]
selected_topics = names(selected_topics)
selected_topics
doc_idx = which(data$Topic %in% selected_topics)
subdoc = data[doc_idx,]
return(subdoc)
}
# Clean corpus
do.preprocess <- function(corpus, stem=FALSE, removeSparseTerms=FALSE) {
# lowercase
corpus = tm_map(corpus, content_transformer(tolower))
# remove stops, punctuation, numbers, whitespaces;
corpus = tm_map(corpus, function(x) removeWords(x, stopwords("english")))
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, stripWhitespace)
## Optional - remove sparse terms
if (removeSparseTerms) {
corpus = tm_map(corpus, removeSparseTerms)
}
# stem
if (stem) {
corpus = tm_map(corpus, stemDocument, language = "english")
}
return (corpus)
}
# NMF
do.nmf <- function(tdm) {
nmf.res = nmf(tdm, 3, "lee")
# estimate target matrix
v.hat = fitted(nmf.res)
# w = n * r term feature matrix
w = basis(nmf.res)
h = coef(nmf.res)
return(h)
}
do.mds <- function(tdm) {
# compute distance matrix
tdm = as.matrix(tdm)
tdm.dist = dist(t(tdm))
tdm.mds = cmdscale(tdm.dist, k = 2)
return(tdm.mds)
}
data = load.data.task()
dim(data)
