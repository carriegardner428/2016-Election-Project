setwd("~/Documents/Data Mining/2016-Election-Project")
library(plyr)     # for recoding data
library(NLP)      # for NLP
library(tm)       # for Corpus()
library(e1071)    # svm
library(rpart)    # decision trees
library(ada)      # for adaboost
library(lsa)
library(NMF)
library(caret)
library(Matrix)
library(SparseM)
library(caret)
install.packages('caret')
library(caret)
library(SparseM)
set.seed(1000)
load.data <- function() {
user.tweets = read.csv('tweets_by_user.csv')
users.party = read.csv('users.csv')
user.tweets[1:3,]
users.party[1:3,]
data = merge(user.tweets, users.party, by=('userID'))
data[1:3,]
# if NAs exist, drop rows
if (any(is.na(data))) {
data = na.omit(data)
}
# Collapse parties
table(data$party)
data$party[data$party=="G"] = "I"
data$party[data$party=="O"] = "I"
data$party[data$party=="L"] = "I"
table(data$party)
dim(data) # 1,866 users (w/o party NAs)
# Change variable types to characters
data$text = as.character(data$text)
data$party = as.character(data$party)
# Show data
data[1:3,]
return (data)
}
load.data.DR <- function() {
# preprocessed in python
tweets = read.csv('DR_users.csv')
tweets = tweets[,-1]
tweets[1:3,]
table(tweets$party) # D:881, R:383
tweets$text = as.character(tweets$text)
tweets$state_code = as.character(tweets$state_code)
# Recode Dems = 0, Reps = 1
tweets$party = revalue(tweets$party, c("D"=0, "R"=1))
any(is.na(tweets$party))
tweets[1:3,]
return(tweets)
}
# Clean corpus
do.preprocess <- function(corpus, removeSparseTerms=FALSE) {
# lowercase
corpus = tm_map(corpus, content_transformer(tolower))
# remove stops, punctuation, numbers, whitespaces;
# Remove everything besides english letters or space
remov = function(x) gsub("[^[:alpha:][:space:]]*", "", x)
corpus <- tm_map(corpus, content_transformer(remov))
# remove URLs & @ mentions
removeURL = function(x) gsub("http[^[:space:]]*", "", x)
removeMention = function(x) gsub("@[:alpha][space:]]*", "", x)
corpus = tm_map(corpus, removeURL)
corpus = tm_map(corpus, removeMention)
myStops = c(stopwords("english"), "https", "http", "debate", "debatenight", "debate night")
corpus = tm_map(corpus, removeWords, myStops)
corpus = tm_map(corpus, stripWhitespace)
## Optional - remove sparse terms
if (removeSparseTerms) {
corpus = tm_map(corpus, removeSparseTerms)
}
# stem
# corpus = tm_map(corpus, stemDocument)
return (corpus)
}
model.predict <- function(X_test, model) {
y_predictions = predict(model, newdata=X_test)
# if probabilties
## probs = predict(model, newdata=X_test)
## y_predictions = as.numeric(probs > .50)
return(y_predictions)
}
model.evaluate <- function(y_predictions, y_test) {
Precision = posPredValue(y_predictions, y_test)
Recall    = sensitivity(y_predictions, y_test)
F1 = (2 * precision * recall) / (precision + recall)
# bind values together
#rownames(col) = c("Precision", "Recall", "F1")
col = rbind(Precision, Recall, F1)
return(col)
}
user.tweets = load.data.DR() # already preprocessed
user.tweets[1:3,]
dim(user.tweets) # 1,866 users
user.corpus = Corpus(VectorSource(user.tweets$text))
inspect(user.corpus[1:3])
user.corpus = do.preprocess(user.corpus)
inspect(user.corpus[1:3])
document_term_matrix = DocumentTermMatrix(user.corpus)
dim(document_term_matrix) # 1,866 observations / 1,264
y = user.tweets$party
length(y)                 # 1,866 observations / 1,264
eval = matrix(, nrow = 3, ncol = 0)
n.obs = nrow(document_term_matrix) # no. of observations
n.obs
s = sample(n.obs)
s
k = 1
k.fold = 3
test.idx = which(s %% k.fold == (k-1) ) # use modular operator
X_train = as.matrix(document_term_matrix[-test.idx,])  # 1,244
X_test  = as.matrix(document_term_matrix[test.idx,])    # 622
y_train = y[-test.idx]                      # 1,244
y_test  = y[test.idx]                        # 622
data.pca = prcomp(X_train, scale=FALSE)
dim(data.pca)
dim(data.pca)
summary(data.pca)
newdat<-data.pca$x[,1:2]
newdat
dim(X_train)
dim(newdat)
summary(data.pca)
dim(train)
train<-data.pca$x[,1:750]
dim(train)
train = cbind(train, y_train)
colnames(train)[ncol(train)] = 'y'
train = as.data.frame(train)
train$y = as.factor(train$y)
model = train(y~., data=train, method="adaboost")
library(e1071)
install.packages('e1071')
library(e1071)
library(ada)
model = train(X_train, y_train, method="adaboost")
model = train(y~., data=train, method="adaboost")
train<-data.pca$x[,1:10]
dim(train)
train = cbind(train, y_train)
colnames(train)[ncol(train)] = 'y'
train = as.data.frame(train)
train$y = as.factor(train$y)
dim(train)
model = train(y~., data=train, method="adaboost")
model = naiveBayes(y~., data=train)
y_predictions = predict(model, newdata=X_test)
y_predictinos
y_predictions
table(y_predictions)
model = naiveBayes(x=X_train, y=y_train)
model = rpart(y_train~X_train, method="class")
y_predictions = predict(model, newdata=X_test)
X_test = as.data.frame(X_test)
y_predictions = predict(model, newdata=X_test)
X_train = as.matrix(document_term_matrix[-test.idx,])  # 1,244
X_test  = as.matrix(document_term_matrix[test.idx,])    # 622
y_train = y[-test.idx]                      # 1,244
y_test  = y[test.idx]                        # 622
dim(X_test)
X_test = as.data.frame(X_test)
dim(X_test)
y_predictions = predict(model, newdata=X_test)
summary(model)
dim(X_train)
y_predictions = predict(model, newdata=X_test)
y_predictinos
y_predictions
dim(y_predictions)
y_predictinos[1:3,]
y_predictions[1:3,]
load.data.DR <- function() {
# preprocessed in python
tweets = read.csv('DR_users.csv') # data is preprocessed in python
tweets = tweets[,-1]
tweets[1:3,]
table(tweets$party) # D:881, R:383
tweets$text = as.character(tweets$text)
tweets$state_code = as.character(tweets$state_code)
# Recode Dems = 0, Reps = 1
tweets$party = revalue(tweets$party, c("D"=0, "R"=1))
any(is.na(tweets$party))
tweets[1:3,]
return(tweets)
}
user.tweets = load.data.DR()
user.corpus = Corpus(VectorSource(user.tweets$text))
document_term_matrix = DocumentTermMatrix(user.corpus)
y = user.tweets$party
dtm = document_term_matrix
S = svd(dtm, nu=2, nv=2)
u = S$u; s = S$d; v = S$v
rownames(u) = docs
v
dim(S)
length(S)
summary(S)
dim(dtm)
u
View(u)
dtm = DocumentTermMatrix(user.corpus, control = list(weights(TfIdf)))
dtm = DocumentTermMatrix(user.corpus, control = list(weight=TfIdf))
dtm = DocumentTermMatrix(user.corpus, control = list(weight=Tfidf))
dtm = DocumentTermMatrix(user.corpus, control = list(weighting=TfIdf))
dtm = DocumentTermMatrix(user.corpus, control = list(weighting=weightTfIdf))
empty.rows = dtm[rowTotals == 0, ]$dimnames[1][1]]
empty.rows = dtm[rowTotals == 0, ]$dimnames[1][[1]]
dtm
row.totals = apply(dtm, 1, sum)
length(row.totals)
dim(dtm)
dim(row.totals)
length(row.totals)
row.totals
document_term_matrix = DocumentTermMatrix(user.corpus)
row.totals = apply(dtm, 1, sum)
length(row.totals)
dtm = DocumentTermMatrix(user.corpus, control = list(weighting=weightTfIdf))
dtm.new = dtm[row.totals > 0, ]
dim(dtm.new)
dim(dtm)
S = svd(dtm, nu=2, nv=2)
S = svd(dtm.new, nu = 10, nv = 10)
y.new   = y[row.totals > 0, ]
length(row.totals)
length(y
length(y)
length(y)
y.new   = y[row.totals > 0, ]
y.new   = y[row.totals > 0]
length(y.new)
dim(dtm.new)# dim 1264, 11897
length(y.new)
u = S$u; s = S$d; v = S$v
summary(S)
d
s
u
dim(u)
